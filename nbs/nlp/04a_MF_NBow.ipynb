{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8c49a7a-e52b-42c7-bdaf-7d7db37e0b12",
   "metadata": {},
   "source": [
    "---\n",
    "title: Text Classification With Keras and Scikit-Learn\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae728d37-c9c0-4504-b081-ababbf7a1c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "We are going to build a model that does classifies customer reviews as positive or negative sentiment, using the [Women's E-Commerce Clothing Reviews Dataset](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews).  We will walk you through how we would organize this task in Metaflow.  Concretely, we will demonstrate the following steps:\n",
    "\n",
    "1. Read data from a parquet file\n",
    "2. Show a branching workflow to record a baseline and train a model in parallel.  \n",
    "3. Evaluate The Model:\n",
    "    - on a holdout set and compare against the baseline\n",
    "    - do a smaoke test\n",
    "4. Retrain the model on the whole dataset if it passsed the criteria and tag it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e372da2-95d6-4be3-a47e-c676ba7ee18f",
   "metadata": {},
   "source": [
    "## Constructing The Metaflow Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75e3b107-901e-46b2-8db0-90846edd261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current\n",
    "\n",
    "class MyFlow(FlowSpec):\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"Read the data\"\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_parquet('train.parquet')\n",
    "        print(f'num of rows: {self.df.shape[0]}')\n",
    "        self.next(self.baseline, self.train)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        baseline_predictions = [1] * self.df.shape[0]\n",
    "        self.base_acc = accuracy_score(self.df.labels, baseline_predictions)\n",
    "        self.base_rocauc = roc_auc_score(self.df.labels, baseline_predictions)\n",
    "        self.next(self.join)\n",
    "\n",
    "    @step\n",
    "    def train(self):\n",
    "        \"Train the model\"\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.utils import set_random_seed\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from model import get_model\n",
    "        set_random_seed(2022)\n",
    "        \n",
    "        self.cv = CountVectorizer(min_df=.005, max_df = .75, stop_words='english', strip_accents='ascii', )\n",
    "        res = self.cv.fit_transform(self.df['review'])\n",
    "        self.model = get_model(len(self.cv.vocabulary_))\n",
    "        self.model.fit(x=res.toarray(), \n",
    "                       y=self.df['labels'],\n",
    "                       batch_size=32, epochs=10, validation_split=.2)\n",
    "\n",
    "        self.next(self.join)\n",
    "        \n",
    "    @step\n",
    "    def join(self, inputs):\n",
    "        \"Compare the model results with the baseline.\"\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import layers, optimizers, regularizers\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        import pandas as pd\n",
    "        \n",
    "        \n",
    "        self.model = inputs.train.model\n",
    "        self.cv = inputs.train.cv\n",
    "        self.train_df = inputs.train.df\n",
    "        self.holdout_df = pd.read_parquet('holdout.parquet')\n",
    "        \n",
    "        self.predictions = self.model.predict(self.cv.transform(self.holdout_df['review']).toarray())\n",
    "        labels = self.holdout_df['labels']\n",
    "        \n",
    "        self.model_acc = accuracy_score(labels, self.predictions > .5)\n",
    "        self.model_rocauc = roc_auc_score(labels, self.predictions)\n",
    "        \n",
    "        print(f'Baseline Acccuracy: {inputs.baseline.base_acc:.2%}')\n",
    "        print(f'Baseline AUC: {inputs.baseline.base_rocauc:.2}')\n",
    "        print(f'Model Acccuracy: {self.model_acc:.2%}')\n",
    "        print(f'Model AUC: {self.model_rocauc:.2}')\n",
    "        self.beats_baseline = self.model_rocauc > inputs.baseline.base_rocauc\n",
    "        print(f'Model beats baseline (T/F): {self.beats_baseline}')\n",
    "        \n",
    "        #smoke test to make sure model is doing the right thing on obvious examples.\n",
    "        _tst_reviews = [\"poor fit its baggy in places where it isn't supposed to be.\",\n",
    "                        \"love it, very high quality and great value\"]\n",
    "        _tst_preds = self.model.predict(self.cv.transform(_tst_reviews).toarray())\n",
    "        self.passed_smoke_test = _tst_preds[0][0] < .5 and _tst_preds[1][0] > .5\n",
    "        print(f'Model passed smoke test (T/F): {self.passed_smoke_test}')\n",
    "        \n",
    "        self.next(self.retrain)\n",
    "\n",
    "    @step\n",
    "    def retrain(self):\n",
    "        \"If model beats the baseline and passes smoke tests, then retrain the model on all available data.\"\n",
    "        if self.beats_baseline and self.passed_smoke_test:\n",
    "            from sklearn.feature_extraction.text import CountVectorizer\n",
    "            import pandas as pd\n",
    "            from model import get_model\n",
    "\n",
    "            all_df = pd.concat([self.train_df, self.holdout_df])\n",
    "            res = self.cv.transform(all_df['review'])\n",
    "            self.final_model = get_model(len(self.cv.vocabulary_))\n",
    "\n",
    "            self.final_model.fit(x=res.toarray(), \n",
    "                                 y=all_df['labels'],\n",
    "                                 batch_size=32, epochs=10, validation_split=.1)\n",
    "            run = Flow(current.flow_name)[current.run_id]\n",
    "            run.add_tag('deployment_candidate')\n",
    "        else:\n",
    "            print('Model was not retrained on full data because of failed smoke test or performance below the baseline.')\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self): ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MyFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d84c0-f9a0-4d30-b2e9-464178830548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mMyFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hamel\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:14.174 \u001b[0m\u001b[1mWorkflow starting (run-id 1658272994170061):\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:14.184 \u001b[0m\u001b[32m[1658272994170061/start/1 (pid 25878)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:15.177 \u001b[0m\u001b[32m[1658272994170061/start/1 (pid 25878)] \u001b[0m\u001b[22mnum of rows: 20377\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:15.294 \u001b[0m\u001b[32m[1658272994170061/start/1 (pid 25878)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:15.304 \u001b[0m\u001b[32m[1658272994170061/baseline/2 (pid 25882)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:15.313 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:16.739 \u001b[0m\u001b[32m[1658272994170061/baseline/2 (pid 25882)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:18.583 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22m2022-07-19 16:23:18.583434: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:18.668 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 1/10\u001b[0m\n",
      "510/510 [==============================] - 1s 1ms/step - loss: 0.3523 - accuracy: 0.8507 - val_loss: 0.2988 - val_accuracy: 0.8754\u001b[0m57 - loss: 0.6811 - accuracy: 0.62\n",
      "\u001b[35m2022-07-19 16:23:19.560 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 2/10\u001b[0m\n",
      "510/510 [==============================] - 1s 988us/step - loss: 0.2945 - accuracy: 0.8818 - val_loss: 0.2956 - val_accuracy: 0.8771\u001b[0m loss: 0.2944 - accuracy: 0.90\n",
      "\u001b[35m2022-07-19 16:23:20.065 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 3/10\u001b[0m\n",
      "510/510 [==============================] - 1s 988us/step - loss: 0.2860 - accuracy: 0.8840 - val_loss: 0.2989 - val_accuracy: 0.8768\u001b[0m loss: 0.2319 - accuracy: 0.96\n",
      "\u001b[35m2022-07-19 16:23:20.569 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 4/10\u001b[0m\n",
      "510/510 [==============================] - 1s 999us/step - loss: 0.2761 - accuracy: 0.8891 - val_loss: 0.2951 - val_accuracy: 0.8803\u001b[0m loss: 0.3847 - accuracy: 0.81\n",
      "\u001b[35m2022-07-19 16:23:21.079 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 5/10\u001b[0m\n",
      "510/510 [==============================] - 1s 988us/step - loss: 0.2676 - accuracy: 0.8956 - val_loss: 0.2991 - val_accuracy: 0.8759\u001b[0m loss: 0.2714 - accuracy: 0.93\n",
      "\u001b[35m2022-07-19 16:23:21.584 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 6/10\u001b[0m\n",
      "510/510 [==============================] - 1s 987us/step - loss: 0.2623 - accuracy: 0.9005 - val_loss: 0.2996 - val_accuracy: 0.8793\u001b[0m loss: 0.3324 - accuracy: 0.84\n",
      "\u001b[35m2022-07-19 16:23:22.088 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 7/10\u001b[0m\n",
      "510/510 [==============================] - 1s 1ms/step - loss: 0.2549 - accuracy: 0.9056 - val_loss: 0.3044 - val_accuracy: 0.8754\u001b[0m - loss: 0.2044 - accuracy: 0.93\n",
      "\u001b[35m2022-07-19 16:23:22.618 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 8/10\u001b[0m\n",
      "510/510 [==============================] - 1s 1ms/step - loss: 0.2472 - accuracy: 0.9109 - val_loss: 0.3137 - val_accuracy: 0.8759\u001b[0m - loss: 0.2832 - accuracy: 0.84\n",
      "\u001b[35m2022-07-19 16:23:23.138 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 9/10\u001b[0m\n",
      "510/510 [==============================] - 1s 989us/step - loss: 0.2376 - accuracy: 0.9134 - val_loss: 0.3151 - val_accuracy: 0.8776\u001b[0m loss: 0.2699 - accuracy: 0.93\n",
      "\u001b[35m2022-07-19 16:23:23.643 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mEpoch 10/10\u001b[0m\n",
      "510/510 [==============================] - 1s 990us/step - loss: 0.2298 - accuracy: 0.9187 - val_loss: 0.3276 - val_accuracy: 0.8744\u001b[0m loss: 0.1719 - accuracy: 0.93\n",
      "\u001b[35m2022-07-19 16:23:24.278 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:24.278 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22m2022-07-19 16:23:24.278132: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:24.306 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[22mWARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:24.869 \u001b[0m\u001b[32m[1658272994170061/train/3 (pid 25883)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-07-19 16:23:24.879 \u001b[0m\u001b[32m[1658272994170061/join/4 (pid 25890)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python flow.py --no-pylint run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9894f-a4fb-424b-aa08-f981760ef9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
