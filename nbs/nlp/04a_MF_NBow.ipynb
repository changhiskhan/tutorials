{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8c49a7a-e52b-42c7-bdaf-7d7db37e0b12",
   "metadata": {},
   "source": [
    "---\n",
    "title: Text Classification With Keras and Scikit-Learn\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df04b2b-eab2-49ac-a591-9b43f39a4aaa",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In order to run this tutorial, we recommend [creating a conda environment defined by a **environment.yml** file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file):\n",
    "\n",
    "---\n",
    "```yaml\n",
    "name: mf-nlp-example\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - jupyterlab=3.2.9\n",
    "  - metaflow\n",
    "  - nbformat=5.4.0\n",
    "  - notebook=6.4.10\n",
    "  - numpy=1.22.0\n",
    "  - pandas=1.3.5\n",
    "  - pip\n",
    "  - pyarrow=8.0.0\n",
    "  - python-dotenv=0.20.0\n",
    "  - ruamel.yaml=0.17.17\n",
    "  - scikit-learn=0.23.2\n",
    "  - tensorflow=2.4.0\n",
    "```\n",
    "\n",
    "You can install this conda environment with the command `conda env create -f environment.yml`, and activate the environment with the command: `conda activate mf-nlp-example`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e5368-de92-43e6-b185-04bb897e929f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "We are going to build a model that does classifies customer reviews as positive or negative sentiment, using the [Women's E-Commerce Clothing Reviews Dataset](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews). Here is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e959f3-7d68-4308-90a1-2c74d61e583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of rows: 20377\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('train.parquet')\n",
    "print(f'num of rows: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73aa7753-0530-4d93-8979-a8c7d9211343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Odd fit: I wanted to love this sweater but the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Very comfy dress: The quality and material of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Fits nicely but fabric a bit thin: I ordered t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Great fit: Love these jeans, fit and style... ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Stretches out, washes poorly. wish i could ret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                             review\n",
       "0       0  Odd fit: I wanted to love this sweater but the...\n",
       "1       1  Very comfy dress: The quality and material of ...\n",
       "2       0  Fits nicely but fabric a bit thin: I ordered t...\n",
       "3       1  Great fit: Love these jeans, fit and style... ...\n",
       "4       0  Stretches out, washes poorly. wish i could ret..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b4fe4-a4b2-4eae-847d-600dda8183c9",
   "metadata": {},
   "source": [
    "## Define The Model\n",
    "\n",
    "In this case we define our model in a seperate file, and define a custom class called `Nbow_Model`.  The model contains two subcomponents: the count vectorizer for preprocessing and the model.  This class facilitates combining these two components together so that we don't have to deal with them seperately.  Here an exaplanation of the various methods in this model:\n",
    "\n",
    "1. `__init__`: this initializes the count vectorizer, a preprocessor that counts the tokens in the text and a nueral network to do the modeling.\n",
    "2. `fit`:  when we call `fit`, we first fit the count vectorizer, followed by the model. \n",
    "3.  `predict`: similarly, when we call `predict`, we need to transform the data with the count vectorizer before making predictions.\n",
    "4. `eval_acc`: calculates model accuracy given a dataset and labels\n",
    "5. `eval_rocauc`: calculates the area under the roc curve given a dataset and labels\n",
    "6. `model_dict`: This exposes of a dictionary that has two components that form this model, the count vectorizer and the nueral network.  We will use this to serialize the model's data into Metaflow. \n",
    "7.  `from_dict`: this allows you to instantiate a NbowModel from a `model_dict` which is useful for de-serializing data in Metaflow.\n",
    "\n",
    "**NB:** Anytime you create your own model library or define models in custom classes, we recommend explicitly defining how you will serialize and load the model.  This will minimize the chances that things will break as your model code changes, by giving you the ability to make sure any new versions of your code are backwards compatible on how to load your model or allow you to deal with serailization/de-serialization accordingly in a way that is transparent to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206ccdd8-c4a6-4f92-9101-69cabde139b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mNbowModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_sz\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Instantiate the CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Define the keras model\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0meval_rocauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"Get Model from dictionary\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnbow_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e372da2-95d6-4be3-a47e-c676ba7ee18f",
   "metadata": {},
   "source": [
    "## Constructing The Metaflow Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66994bb8-b3c4-449f-b0bb-f5b56e021cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    " We will walk you through how we would organize this task in Metaflow.  Concretely, we will demonstrate the following steps:\n",
    "\n",
    "1. **Read data from a parquet file** in the `start` step.\n",
    "    - We use pandas to read `train.parquet`\n",
    "    - Notice how we are assigning the training data to `self.df` this stores the data as an artifact in Metaflow, which means it will be versioned and saved in the artifact store for later retrieval.  Furthermore, this allows you to pass data to another step.\n",
    "    - We log the number of rows in the data.  It is always a good idea to log information about your dataset for debugging.\n",
    "2. **Show a branching workflow to create a baseline and candidate model in parallel**  in the `baseline` and `train` steps.\n",
    "    - When we call `self.next(self.baseline, self.train)`,  this creates a [branching flow](https://docs.metaflow.org/metaflow/basics#branch) that will allow the `baseline` and  `train` steps to run in parallel.\n",
    "    - The `baseline` step records the performance metrics (accuracy and roc auc score) that result from classifying all examples with the majority class.  This will be our baseline against which we evaluate our model.\n",
    "    - The `train` step uses a nueral bag of words model to train a text classifier.  We serialize this model in a special way by getting the `model_dict` property of our custom model.\n",
    "3. **Evaluate The Model** in the `join` step:\n",
    "    - Anytime you have branching in your flow, you [must have a join step](https://docs.metaflow.org/metaflow/basics#branch). join step allows you to access data from your vairous branches via the `inputs` parameter.  For more details, see the section about [data flow](https://docs.metaflow.org/metaflow/basics#data-flow-through-the-graph). Furthermore we evaluate your model on the holdout set and log the performance metrics using print statements.\n",
    "4. **The model is tagged as a `deployment_candidate`** in the `end` step, depneding on if it meets our performance criteria. \n",
    "    - First we do a smoke test by testing the model on a few obvious examples where we expect the model to make good predictions.\n",
    "    - If the model beats the baseline and passes the smoke test, we tag it as a deployment candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e3b107-901e-46b2-8db0-90846edd261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current\n",
    "\n",
    "class NLPFlow(FlowSpec):\n",
    "        \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"Read the data\"\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_parquet('train.parquet')\n",
    "        print(f'num of rows: {self.df.shape[0]}')\n",
    "        self.next(self.baseline, self.train)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        baseline_predictions = [1] * self.df.shape[0]\n",
    "        self.base_acc = accuracy_score(self.df.labels, baseline_predictions)\n",
    "        self.base_rocauc = roc_auc_score(self.df.labels, baseline_predictions)\n",
    "        self.next(self.join)\n",
    "\n",
    "    @step\n",
    "    def train(self):\n",
    "        \"Train the model\"\n",
    "        from model import Nbow_Model\n",
    "        model = Nbow_Model(vocab_sz=750)\n",
    "        model.fit(X=self.df['review'], y=self.df['labels'])\n",
    "        self.model_dict = model.model_dict #save model\n",
    "        self.next(self.join)\n",
    "        \n",
    "    @step\n",
    "    def join(self, inputs):\n",
    "        \"Compare the model results with the baseline.\"\n",
    "        import pandas as pd\n",
    "        from model import NbowModel\n",
    "        self.model_dict = inputs.train.model_dict\n",
    "        self.train_df = inputs.train.df\n",
    "        self.holdout_df = pd.read_parquet('holdout.parquet')\n",
    "        model = NbowModel.from_dict(self.model_dict)\n",
    "        \n",
    "        self.model_acc = model.eval_acc(X=self.holdout_df['review'], labels=self.holdout_df['labels'])\n",
    "        self.model_rocauc = model.eval_rocauc(X=self.holdout_df['review'], labels=self.holdout_df['labels'])\n",
    "        \n",
    "        print(f'Baseline Acccuracy: {inputs.baseline.base_acc:.2%}')\n",
    "        print(f'Baseline AUC: {inputs.baseline.base_rocauc:.2}')\n",
    "        print(f'Model Acccuracy: {self.model_acc:.2%}')\n",
    "        print(f'Model AUC: {self.model_rocauc:.2}')\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"Tags model as a deployment candidate if it beats the baseline and passes smoke tests.\"\n",
    "        self.beats_baseline = self.model_rocauc > inputs.baseline.base_rocauc\n",
    "        print(f'Model beats baseline (T/F): {self.beats_baseline}')\n",
    "        #smoke test to make sure model is doing the right thing on obvious examples.\n",
    "        _tst_reviews = [\"poor fit its baggy in places where it isn't supposed to be.\",\n",
    "                        \"love it, very high quality and great value\"]\n",
    "        _tst_preds = model.predict(_tst_reviews)\n",
    "        self.passed_smoke_test = _tst_preds[0][0] < .5 and _tst_preds[1][0] > .5\n",
    "        print(f'Model passed smoke test (T/F): {self.passed_smoke_test}')\n",
    "        \n",
    "        if self.beats_baseline and self.passed_smoke_test:\n",
    "            run = Flow(current.flow_name)[current.run_id]\n",
    "            run.add_tag('deployment_candidate')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NLPFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1f87d-3a4d-4d5d-a355-2f256b4f812c",
   "metadata": {},
   "source": [
    "We can execute the flow like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4d84c0-f9a0-4d30-b2e9-464178830548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNLPFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hamel\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[22m    flow.py:54:50: E0602: Undefined variable 'inputs' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    flow.py:59:21: E0602: Undefined variable 'model' (undefined-variable)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[1m    Pylint is not happy\u001b[0m\u001b[22m:\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    \u001b[0m\u001b[31m\u001b[1mFix Pylint warnings listed above or say --no-pylint.\u001b[0m\u001b[22m\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m\u001b[K\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "!python flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef18b4e-6d46-4219-9329-6c7ffd89d55e",
   "metadata": {},
   "source": [
    "## Using The Model In Production\n",
    "\n",
    "After you have trained a model in Metaflow, you may want to utilize this model to make predictions or for futher testing.  There are two common patterns for this: (1) Retrieve the model from Metaflow in an external system (2) Have another flow that does predictions.  We illustrate both examples here:\n",
    "\n",
    "### 1. Retrieve Model From Metaflow To Use In External Systems\n",
    "\n",
    "You can now retrieve the model tagged as a `deployment_candidate` outside Metaflow, so you can use this in whatever downstream application you want, or even just for ad-hoc testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e9894f-a4fb-424b-aa08-f981760ef9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "import pandas as pd\n",
    "from model import NbowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b34b744-225b-490a-8fb8-a76bcdc1f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.read_parquet('predict.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f3b56b-292b-441f-9b1a-e5edb149866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_successful_run(flow_nm, tag):\n",
    "    \"Gets the latest successfull run for a flow with a specific tag.\"\n",
    "    for r in Flow(flow_nm).runs(tag):\n",
    "        if r.successful: return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79231358-edef-4560-801e-8fac811eaeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 13:18:43.080782: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "run = get_latest_successful_run('NLPFlow', 'deployment_candidate')\n",
    "model = NbowModel.from_dict(run.data.model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264fc51-acdc-4458-b949-f71da54909c0",
   "metadata": {},
   "source": [
    "Now that we have retrieved the model using the tag we can use it to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902f5d50-8490-4f70-b3ee-79316965851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99896604],\n",
       "       [0.9890883 ],\n",
       "       [0.99934345],\n",
       "       ...,\n",
       "       [0.99978065],\n",
       "       [0.9995602 ],\n",
       "       [0.5956297 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(predict_df['review'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7765d5-7b22-4fef-94a1-5e688af8d358",
   "metadata": {},
   "source": [
    "You can write these predictions to a parquet file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ce452f-6ec2-4cba-b074-7c8a929d331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "pa_tbl = pa.table({\"data\": preds.squeeze()})\n",
    "pa.parquet.write_table(pa_tbl, \"sentiment_predictions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a8580-dcb6-4d1c-8442-1c21eacf91d4",
   "metadata": {},
   "source": [
    "### 2. Use a Flow To Make Predictions\n",
    "\n",
    "You may want to do batch predictions in a Flow as well.  In this flow, we will perform the following steps:\n",
    "\n",
    "1. Get the latest deployment candidate using the Metaflow API in the `start` step.\n",
    "2. Make predictions with our deployment candidate on a new dataset and write that to a parquet file in the `end` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc83ba1-988f-4ca2-9a19-467806308dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predflow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Flow, current\n",
    "\n",
    "class NLPredictionFlow(FlowSpec):\n",
    "    \n",
    "    def get_latest_successful_run(self, flow_nm, tag):\n",
    "        \"Gets the latest successfull run for a flow with a specific tag.\"\n",
    "        for r in Flow(flow_nm).runs(tag):\n",
    "            if r.successful: return r\n",
    "        \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"Get the latest deployment candidate that is from a successfull run\"\n",
    "        self.deploy_run = self.get_latest_successful_run('NLPFlow', 'deployment_candidate')\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"Make predictions\"\n",
    "        from model import NbowModel\n",
    "        import pandas as pd\n",
    "        import pyarrow as pa\n",
    "        new_reviews = pd.read_parquet('predict.parquet')['review']\n",
    "        \n",
    "        # Make predictions\n",
    "        model = NbowModel.from_dict(self.deploy_run.data.model_dict)\n",
    "        predictions = model.predict(new_reviews)\n",
    "        print(f'Writing predictions to parquet: {predictions.shape[0]:,} rows')\n",
    "        pa_tbl = pa.table({\"data\": predictions.squeeze()})\n",
    "        pa.parquet.write_table(pa_tbl, \"sentiment_predictions.parquet\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    NLPredictionFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b6269d-f70b-41d5-b9ee-363263506e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNLPredictionFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hamel\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:44.210 \u001b[0m\u001b[1mWorkflow starting (run-id 1658434724205954):\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:44.219 \u001b[0m\u001b[32m[1658434724205954/start/1 (pid 57802)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:45.026 \u001b[0m\u001b[32m[1658434724205954/start/1 (pid 57802)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:45.033 \u001b[0m\u001b[32m[1658434724205954/end/2 (pid 57806)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:47.556 \u001b[0m\u001b[32m[1658434724205954/end/2 (pid 57806)] \u001b[0m\u001b[22m2022-07-21 13:18:47.556819: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:47.908 \u001b[0m\u001b[32m[1658434724205954/end/2 (pid 57806)] \u001b[0m\u001b[22mWriting predictions to parquet: 2,264 rows\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:48.190 \u001b[0m\u001b[32m[1658434724205954/end/2 (pid 57806)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:48.191 \u001b[0m\u001b[32m[1658434724205954/end/2 (pid 57806)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-07-21 13:18:48.192 \u001b[0m\u001b[1mDone!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python predflow.py --no-pylint --datastore local --metadata local run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d363-ecbb-4784-8c50-6c61c36a164f",
   "metadata": {},
   "source": [
    "### Further Discussion\n",
    "\n",
    "This is a very simple example that will also run on your laptop.  However, for production use cases you may want to use [@conda](https://docs.metaflow.org/metaflow/dependencies#managing-dependencies-with-conda-decorator) for dependency management, [@batch](https://docs.metaflow.org/v/r/metaflow/scaling#using-aws-batch) or [@kubernetes](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes) for remote execution, and [@schedule](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions#scheduling-a-flow) to schedule jobs to run periodically.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
