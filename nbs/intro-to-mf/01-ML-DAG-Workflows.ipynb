{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f612476-9d5b-4fe3-8e71-182848880c94",
   "metadata": {},
   "source": [
    "# Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e2b81-3cc1-4af2-816d-c319f1883154",
   "metadata": {},
   "source": [
    "## Before this lesson\n",
    "* Clone this repository\n",
    "    * `git clone https://github.com/outerbounds/tutorials.git`\n",
    "* [Install conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)\n",
    "* Install requirements\n",
    "    * From the root of this repository run `conda env create -f env.yml`\n",
    "* Open this notebook\n",
    "    * `jupyter notebook nbs/intro-to-mf/01-ML-DAG-Workflows.ipynb`\n",
    "    \n",
    "    \n",
    "    \n",
    "[HBA: lets discuss whether we need learners to do the above or if there are other ways (if we do, let's have a super minimal env and use mamba also?). the current tutorials are cool because there's minimal friction in running them]\n",
    "\n",
    "\n",
    "[HBA: I think we're homing in on a good structure for lessons and episodes but next let's be really explicit; e.g. each sub-episode has i) showcasing, ii) brief context/description, iii) the flow, iv) execution, then v) interpretation or closing. something like that. whatever we decide works best but let's align and be explicit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa47871-1af7-4e90-a822-b076d03f9ac6",
   "metadata": {},
   "source": [
    "## Episode 1: Metaflow Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6cffe-a86f-4668-bf9e-20a069bbd7e6",
   "metadata": {},
   "source": [
    "[HBA: current prose is a first pass so don't overindex feedback on it; we'll also link out to how tos and core concepts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf235-ac88-4ad6-ae6a-03db5efbb695",
   "metadata": {},
   "source": [
    "### Showcasing\n",
    "- DAGs and `metaflow.FlowSpec`\n",
    "- Decorators and `metaflow.step`\n",
    "- Metaflow Artifacts\n",
    "- Metaflow Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035f4f4-a827-4998-9473-56ac3df2b33a",
   "metadata": {},
   "source": [
    "Metaflow is a tool for data scientists. It helps you efficiently access the environments, data, and infrastructure you need to get data science jobs done. Metaflow helps you realize these benefits by providing a consistent structure to your data science workflows. This structure is a directed acyclic graph (DAG). In addition to providing mechnaisms to encode your workflow as a DAG, Metaflow offers many design patterns relevant to machine learning workflows. The next sections of this page will introduce you to the Metaflow DAG, show you how to structure a Metaflow flow, and then introduce several common Metaflow constructs that leverage this structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f9187-2e0f-4626-aaab-9c18942b8f4d",
   "metadata": {},
   "source": [
    "### 1a. FlowSpec and step\n",
    "\n",
    "The DAG is a graph with no cycles and edges that only point in one direction. In Metaflow, you can create a DAG by sub-classing the `metaflow.FlowSpec` object. The nodes of the DAG correspond to functions of the `FlowSpec` class that are annotated with the `@step` decorator. Every flow you create must contain a `start` and `end` function. Here you can see an example of the minimal Metaflow flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f87e141c-6976-4a51-bab5-97ea2609b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minimum_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile minimum_flow.py\n",
    "from metaflow import FlowSpec, step\n",
    "\n",
    "class MinimumFlow(FlowSpec):\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Flow is done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MinimumFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384903a-4424-478d-bd73-934cbd0cfe78",
   "metadata": {},
   "source": [
    "[HBA: Let's add a note about writing flows in scripts using your fave text editor and then executing from CLI, as we do below]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af34b30-728d-470e-b85c-6b50966ac734",
   "metadata": {},
   "source": [
    "Notice that the functions take the `MinimumFlow` object itself as an argument, and use the `self` entity to create the structure of the DAG. This is doing by using `self.next(self.next_step)` at the end of a `step`.\n",
    "\n",
    "The flow can be run from the command line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f8885b1-f2d9-45da-a91e-da6d67301678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mMinimumFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.040 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418634035942):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.097 \u001b[0m\u001b[32m[1659418634035942/start/1 (pid 4271)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.461 \u001b[0m\u001b[32m[1659418634035942/start/1 (pid 4271)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.469 \u001b[0m\u001b[32m[1659418634035942/end/2 (pid 4275)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.793 \u001b[0m\u001b[32m[1659418634035942/end/2 (pid 4275)] \u001b[0m\u001b[22mFlow is done!\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.840 \u001b[0m\u001b[32m[1659418634035942/end/2 (pid 4275)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:14.841 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python minimum_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f91ab-0d2a-41a2-b71d-47a16ac31aec",
   "metadata": {},
   "source": [
    "[HBA: can we include some notes on how to read the above output, what it means, and why it is useful?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facd5e6-f329-4896-a73d-9e7275556bc3",
   "metadata": {},
   "source": [
    "For now, just remember that the general command is `python <FLOW SCRIPT> run`. Later you will see more ways to interact with your flows from the command line. Stay tuned! \n",
    "\n",
    "Bonus points: Try to add another step to the previous flow in addition to `start` and `end`, don't forget `@step`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e61c2-edac-4f28-bc5e-eda3cd4a6959",
   "metadata": {},
   "source": [
    "### 1b. Decorators\n",
    "\n",
    "Using Metaflow requires the use of decorators. In Python, a decorator is a function that takes another function and extends its behavior without the need to modify it directly. In episode 1 part a. you saw Metaflow's `@step` decorators in action. This is just the beginning. There are many decorators built-into Metaflow and built as plugins by community members. You don't have to understand all of these now, but keep in mind that there are a wide-variety of decorators you can use.\n",
    "\n",
    "For example, at the step-level there are decorators for use cases including:\n",
    "* `@conda` for dependency management of a single step's environment\n",
    "* `@batch` or `@kubernetes` to run a step on AWS Batch or Kubernetes\n",
    "\n",
    "There are also flow-level decorators such as:\n",
    "* `@conda_base` for dependency management of a flow's environment\n",
    "* `@schedule` to run jobs automatically on a production orchestrator\n",
    "\n",
    "You can view a list of all step decorators [here](https://docs.metaflow.org/api/step-decorators) and all flow decorators [here](https://docs.metaflow.org/api/flow-decorators)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e0e0f-cc7f-47e6-96f1-abb2966bdbc3",
   "metadata": {},
   "source": [
    "[HBA: this sub-episode doesn't have any executable code or flows written so let's consider adding some or merging this into 1a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9abe04-ceb6-418e-945d-5d2d74ccb60e",
   "metadata": {},
   "source": [
    "### 1c. Flow Artifacts\n",
    "\n",
    "[HBA: let's make sure to explicitly define what an artifact is]\n",
    "[HBA: part of me wonders whether we want to spice up this sub-episode by having real data or an actual ML model. Just an idea]\n",
    "\n",
    "Machine learning is all about the data. In this section, we are referring to how the state of flow artifacts change. This is done using the `self` keyword that refers to your flow object. When you use this keyword to store data artifact values, Metaflow automatically serializes this data and makes it usable across the rest of the downstream steps in your flow. This is especially useful when you run different steps of the flow on different computers. \n",
    "\n",
    "Here is a flow that shows using `self` after creating and updating an artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "baafb904-e8ba-4f7f-8208-a5ecb7581f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting artifact_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifact_flow.py\n",
    "from metaflow import FlowSpec, step\n",
    "\n",
    "class ArtifactFlow(FlowSpec):\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        self.data_artifact = 1 # create `data_artifact`\n",
    "        self.next(self.middle)\n",
    "        \n",
    "    @step\n",
    "    def middle(self):\n",
    "        self.data_artifact = 3 # update `data_artifact`\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        self.data_artifact += 1 # update `data_artifact`\n",
    "        print(\"Artifact is {}\".format(self.data_artifact))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ArtifactFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0e53998-f3f8-420d-9bb5-87d3f9518c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mArtifactFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:17.622 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418637598266):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:17.629 \u001b[0m\u001b[32m[1659418637598266/start/1 (pid 4281)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:17.992 \u001b[0m\u001b[32m[1659418637598266/start/1 (pid 4281)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.000 \u001b[0m\u001b[32m[1659418637598266/middle/2 (pid 4284)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.366 \u001b[0m\u001b[32m[1659418637598266/middle/2 (pid 4284)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.374 \u001b[0m\u001b[32m[1659418637598266/end/3 (pid 4287)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.702 \u001b[0m\u001b[32m[1659418637598266/end/3 (pid 4287)] \u001b[0m\u001b[22mArtifact is 4\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.748 \u001b[0m\u001b[32m[1659418637598266/end/3 (pid 4287)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:18.748 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python artifact_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfee23-e010-41ef-a414-e923a1446636",
   "metadata": {},
   "source": [
    "Note that although this artifact pattern can deal with many cases, this pattern is not for storing big data. In the case where you have a large dataset (e.g., a training dataset of many images) you may want to consider using [Metaflow's S3 utilities](https://docs.metaflow.org/api/S3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38ec37-c9fd-4619-876c-f38fb403a158",
   "metadata": {},
   "source": [
    "### 1d. Flow Parameters\n",
    "\n",
    "You can also pass data into flows as a `Parameter`. You can use these to parameterize any aspect of your flow. The parameters can be set to a default and give you the option to override this from the command line when you (or a scheduler) runs the flow. Here is the `MinimumFlow` example adding a `Parameter` into the mix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883bc93b-15c9-4f19-91a9-9979fb5ddb11",
   "metadata": {},
   "source": [
    "[HBA: it's not quite clear why this would be useful yet. We could say a few words about when you would want to use and/or have an actual ML example (see above also)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5011c9a-be24-46be-9296-dae8d62f1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parameter_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parameter_flow.py\n",
    "from metaflow import FlowSpec, step, Parameter\n",
    "\n",
    "class ParameterizedFlow(FlowSpec):\n",
    "    \n",
    "    my_param = Parameter('my-param', default=999)\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Parameter value is {}\".format(self.my_param))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ParameterizedFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf427c2-a1b0-49b5-823f-e63270bcf960",
   "metadata": {},
   "source": [
    "The flow can be run like before and the default will be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17911e1f-99ff-4326-b1b4-b8ac26cd1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mParameterizedFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:22.613 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418642608733):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:22.620 \u001b[0m\u001b[32m[1659418642608733/start/1 (pid 4293)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:22.990 \u001b[0m\u001b[32m[1659418642608733/start/1 (pid 4293)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:22.998 \u001b[0m\u001b[32m[1659418642608733/end/2 (pid 4297)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:23.323 \u001b[0m\u001b[32m[1659418642608733/end/2 (pid 4297)] \u001b[0m\u001b[22mParameter value is 999\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:23.369 \u001b[0m\u001b[32m[1659418642608733/end/2 (pid 4297)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:23.370 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python parameter_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf943a-322c-4c3f-8087-a60d17ed50c7",
   "metadata": {},
   "source": [
    "Or you can override the default value at run time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16e91a03-9d56-4b37-8a9d-fed03c0526f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mParameterizedFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:25.853 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418645848463):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:25.861 \u001b[0m\u001b[32m[1659418645848463/start/1 (pid 4304)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:26.228 \u001b[0m\u001b[32m[1659418645848463/start/1 (pid 4304)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:26.235 \u001b[0m\u001b[32m[1659418645848463/end/2 (pid 4307)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:26.565 \u001b[0m\u001b[32m[1659418645848463/end/2 (pid 4307)] \u001b[0m\u001b[22mParameter value is 123\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:26.611 \u001b[0m\u001b[32m[1659418645848463/end/2 (pid 4307)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:26.611 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python parameter_flow.py run --my-param 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac1255-8924-42ae-8e3e-ae4dadea17a4",
   "metadata": {},
   "source": [
    "## Episode 2. Running Flows\n",
    "\n",
    "### Showcasing\n",
    "- Machine learning workflows\n",
    "- Branching\n",
    "\n",
    "This episode demonstrates a more realistic machine learning job. You will see a flow that trains two models on the same classification task, one Random Forest and one Gradient Boosted Trees model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cf3b3-e4bf-4828-8f82-1b6a2cdbfb62",
   "metadata": {},
   "source": [
    "[HBA: we could split this into 2-3 sub-episodes? e.g. ML model then branching? the previous episode contains 4 sections and this has one]\n",
    "[HBA: perhaps have \"Machine Learning\" in title of episode?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f3813-9c7a-48a0-a43c-6ea3daae9637",
   "metadata": {},
   "source": [
    "### Tree Models Flow\n",
    "\n",
    "[HBA: this h3 could have a few more words]\n",
    "\n",
    "The flow has the following structure:\n",
    "* Parameter values are defined in beginning of the class.\n",
    "    * Defaults can be overridden using command line arguments as shown in episode 1d.\n",
    "* The `start` step loads and splits a dataset to be used in downstream tasks.\n",
    "    * The dataset for this task is small, so we can store it in `self` without introducing much copying and storage overhead.\n",
    "    * Notice that this step calls two downstream steps in `self.next(self.train_rf, self.train_xgb)`. This is called branching in Metaflow. These \"branching\" steps are run in parallel. \n",
    "* The `train_rf` step fits a `sklearn.ensemble.RandomForestClassifier` for the classification task. \n",
    "* The `train_xgb` step fits a `xgboost.XGBClassifier` for the classification task. \n",
    "* The `score` step evaluates each classifier on a held out dataset for testing.\n",
    "* The `end` step prints the accuracy scores for each classifier.\n",
    "\n",
    "\n",
    "[HBA: the following flow is long; one aspect it will be important for us to discuss is whether we want ALL the code on the tutorial page or only some of it and then to point people to the repo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b4f7f63-c19b-40b7-beeb-49ba4e623f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tree_models_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tree_models_flow.py\n",
    "from metaflow import FlowSpec, step, Parameter\n",
    "\n",
    "class TreeModelsFlow(FlowSpec):\n",
    "\n",
    "    test_size = Parameter(\"tst-sz\", default=0.2)\n",
    "    random_state = Parameter(\"seed\", default=42)\n",
    "    n_estimators = Parameter(\"n-est\", default=10)\n",
    "    min_samples_split = Parameter(\"min-samples\", default=2)\n",
    "    eval_metric = Parameter(\"eval-metric\", default='mlogloss')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        iris = datasets.load_iris()\n",
    "        self.X = iris['data']\n",
    "        self.y = iris['target']\n",
    "        data = train_test_split(self.X, self.y, \n",
    "                                test_size=self.test_size, \n",
    "                                random_state=self.random_state)\n",
    "        self.X_train = data[0]\n",
    "        self.X_test = data[1]\n",
    "        self.y_train = data[2]\n",
    "        self.y_test = data[3]\n",
    "        self.next(self.train_rf, self.train_xgb)\n",
    "\n",
    "    @step\n",
    "    def train_rf(self):\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        self.clf = RandomForestClassifier(n_estimators=self.n_estimators,\n",
    "                                          min_samples_split=self.min_samples_split, \n",
    "                                          random_state=self.random_state)\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.next(self.score)\n",
    "\n",
    "    @step\n",
    "    def train_xgb(self):\n",
    "        from xgboost import XGBClassifier\n",
    "        self.clf = XGBClassifier(n_estimators=self.n_estimators,\n",
    "                                 random_state=self.random_state,\n",
    "                                 eval_metric=self.eval_metric,\n",
    "                                 use_label_encoder=False)\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.next(self.score)\n",
    "\n",
    "    @step\n",
    "    def score(self, inputs):\n",
    "        self.merge_artifacts(inputs, include=[\"X_test\", \"y_test\"])\n",
    "        self.accuracies = [\n",
    "            train_step.clf.score(self.X_test, self.y_test)\n",
    "            for train_step in inputs\n",
    "        ]\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        self.model_names = [\"Random Forest\", \"XGBoost\"]\n",
    "        for name, acc in zip(self.model_names, self.accuracies):\n",
    "            print(\"{} Model Accuracy: {}%\".format(\n",
    "                name, round(100*acc, 3)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TreeModelsFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18b6e9c4-ea54-4a84-aab9-c70c4feb33a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTreeModelsFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:31.315 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418651310098):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:31.326 \u001b[0m\u001b[32m[1659418651310098/start/1 (pid 4313)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:32.356 \u001b[0m\u001b[32m[1659418651310098/start/1 (pid 4313)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:32.366 \u001b[0m\u001b[32m[1659418651310098/train_rf/2 (pid 4316)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:32.377 \u001b[0m\u001b[32m[1659418651310098/train_xgb/3 (pid 4317)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:33.312 \u001b[0m\u001b[32m[1659418651310098/train_rf/2 (pid 4316)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:33.681 \u001b[0m\u001b[32m[1659418651310098/train_xgb/3 (pid 4317)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:33.690 \u001b[0m\u001b[32m[1659418651310098/score/4 (pid 4323)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:34.712 \u001b[0m\u001b[32m[1659418651310098/score/4 (pid 4323)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:34.722 \u001b[0m\u001b[32m[1659418651310098/end/5 (pid 4326)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:35.108 \u001b[0m\u001b[32m[1659418651310098/end/5 (pid 4326)] \u001b[0m\u001b[22mRandom Forest Model Accuracy: 100.0%\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:35.159 \u001b[0m\u001b[32m[1659418651310098/end/5 (pid 4326)] \u001b[0m\u001b[22mXGBoost Model Accuracy: 100.0%\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:35.160 \u001b[0m\u001b[32m[1659418651310098/end/5 (pid 4326)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:37:35.161 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python tree_models_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cf392-360e-46c1-bde4-c9f1ad3c3bc0",
   "metadata": {},
   "source": [
    "## 3. Visualizing Flows with Cards\n",
    "\n",
    "### Showcasing\n",
    "- Neural net workflows\n",
    "- Metaflow cards\n",
    "\n",
    "Data visualization is a crucial aspect of communicating machine learning results successfully. Metaflow offers several utilities to help you in this regard. The quickest way to start viewing your flow artifacts and flow structures is to use cards. These are step-level entities that let you visualize plots, data tables, HTML, and more in your browser. You can read more about cards [here](https://docs.metaflow.org/metaflow/visualizing-results#what-are-cards). In this episode, you will see a new model type trained on same classification task from episode 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f50f2c-cfe9-43e6-93b1-c7e25fdeaa6c",
   "metadata": {},
   "source": [
    "### Neural Net Flow\n",
    "\n",
    "The flow has the following structure:\n",
    "* Parameter values are defined in beginning of the class\n",
    "    * Defaults can be overridden using ommand line arguments as shown in episode 1d.\n",
    "* The `start` step loads and splits a dataset to be used in downstream tasks.\n",
    "* The `scale_features` step normalizes the feature data. \n",
    "* The `visualize_feature_distributions` makes a `matplotlib.Figure` that is appended to the step's card.\n",
    "    * Later, we will use this step name in the command line to visualize the card for this step.\n",
    "    * The figure produced compares the training and testing features before and after scaling.\n",
    "* The `train` step fits the neural net.\n",
    "* The `score` step evaluated the model accuracy.\n",
    "* The `end` step prints the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd74d33-1f27-47b5-a292-568daf8e6c2e",
   "metadata": {},
   "source": [
    "[HBA: same question as above about including all code in tutorial or not]\n",
    "\n",
    "[HBA: perhaps include utils.py for boilerplate?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61522d8a-e970-4de7-9c89-cbc8fa993237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neural_net_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neural_net_flow.py\n",
    "from metaflow import FlowSpec, step, Parameter, card, current\n",
    "from metaflow.cards import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_model(hidden_layer_dim, meta):\n",
    "    # meta is a scikeras argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(n_features_in_, \n",
    "                                 input_shape=X_shape_[1:]))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(hidden_layer_dim))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(n_classes_))\n",
    "    model.add(keras.layers.Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "class NeuralNetFlow(FlowSpec):\n",
    "\n",
    "    test_size = Parameter(\"tst-sz\", default=0.2)\n",
    "    random_state = Parameter(\"seed\", default=42)\n",
    "    hidden_layer_dim = Parameter(\"hidden-dim\", default=100)\n",
    "    epochs = Parameter(\"epochs\", default=200)\n",
    "    loss_fn = Parameter(\"loss\", default='categorical_crossentropy')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        data = train_test_split(self.X, self.y, \n",
    "                                test_size=self.test_size, \n",
    "                                random_state=self.random_state)\n",
    "        self.X_train = data[0]\n",
    "        self.X_test = data[1]\n",
    "        self.y_train = data[2]\n",
    "        self.y_test = data[3]\n",
    "        self.next(self.scale_features)\n",
    "\n",
    "    @step\n",
    "    def scale_features(self):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        self.X_train_scaled = scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = scaler.transform(self.X_test)\n",
    "        self.next(self.visualize_feature_distributions)\n",
    "\n",
    "    @card\n",
    "    @step\n",
    "    def visualize_feature_distributions(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        n_features = self.X_train.shape[1]\n",
    "        assert n_features == self.X_test.shape[1], \"Train and test feature dimensions are not the same!\"\n",
    "        feature_datasets = [self.X_train, self.X_train_scaled, self.X_test, self.X_test_scaled]\n",
    "        n_bins = 10\n",
    "        fig, axs = plt.subplots(len(feature_datasets), n_features, figsize=(16,16))\n",
    "        for i,data in enumerate(feature_datasets):\n",
    "            for j in range(n_features):\n",
    "                axs[i,j].hist(data[:, i], bins=n_bins)\n",
    "                axs[i,j].set_title(\"X train - {}\".format(self.iris['feature_names'][i]))\n",
    "        current.card.append(Image.from_matplotlib(fig))\n",
    "        self.next(self.train)\n",
    "\n",
    "    @step\n",
    "    def train(self):\n",
    "        from scikeras.wrappers import KerasClassifier\n",
    "        self.clf = KerasClassifier(build_model, \n",
    "                                   loss=self.loss_fn,\n",
    "                                   hidden_layer_dim=self.hidden_layer_dim,\n",
    "                                   epochs=self.epochs,\n",
    "                                   verbose=0)\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.next(self.score)\n",
    "\n",
    "    @step\n",
    "    def score(self):\n",
    "        self.accuracy = self.clf.score(self.X_test, self.y_test)\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Neural Net Model Accuracy: {}%\".format(round(100*self.accuracy, 3)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NeuralNetFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc3379-21be-416b-aaed-695eb0ac12ea",
   "metadata": {},
   "source": [
    "The flow can be run in the same way as usual when using cards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b904ed42-5965-42e7-a7e5-5331d824ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNeuralNetFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:01.522 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418681517256):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:01.540 \u001b[0m\u001b[32m[1659418681517256/start/1 (pid 4334)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:04.476 \u001b[0m\u001b[32m[1659418681517256/start/1 (pid 4334)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:04.489 \u001b[0m\u001b[32m[1659418681517256/scale_features/2 (pid 4337)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:06.813 \u001b[0m\u001b[32m[1659418681517256/scale_features/2 (pid 4337)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:06.826 \u001b[0m\u001b[32m[1659418681517256/visualize_feature_distributions/3 (pid 4340)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:12.755 \u001b[0m\u001b[32m[1659418681517256/visualize_feature_distributions/3 (pid 4340)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:12.767 \u001b[0m\u001b[32m[1659418681517256/train/4 (pid 4346)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:14.863 \u001b[0m\u001b[32m[1659418681517256/train/4 (pid 4346)] \u001b[0m\u001b[22m2022-08-02 00:38:14.862861: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:15.729 \u001b[0m\u001b[32m[1659418681517256/train/4 (pid 4346)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:15.729 \u001b[0m\u001b[32m[1659418681517256/train/4 (pid 4346)] \u001b[0m\u001b[22m2022-08-02 00:38:15.728889: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:16.305 \u001b[0m\u001b[32m[1659418681517256/train/4 (pid 4346)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:16.317 \u001b[0m\u001b[32m[1659418681517256/score/5 (pid 4349)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:18.399 \u001b[0m\u001b[32m[1659418681517256/score/5 (pid 4349)] \u001b[0m\u001b[22m2022-08-02 00:38:18.399525: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:18.706 \u001b[0m\u001b[32m[1659418681517256/score/5 (pid 4349)] \u001b[0m\u001b[22mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:18.706 \u001b[0m\u001b[32m[1659418681517256/score/5 (pid 4349)] \u001b[0m\u001b[22m2022-08-02 00:38:18.706383: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:19.248 \u001b[0m\u001b[32m[1659418681517256/score/5 (pid 4349)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:19.260 \u001b[0m\u001b[32m[1659418681517256/end/6 (pid 4352)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:21.063 \u001b[0m\u001b[32m[1659418681517256/end/6 (pid 4352)] \u001b[0m\u001b[22mNeural Net Model Accuracy: 100.0%\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:21.332 \u001b[0m\u001b[32m[1659418681517256/end/6 (pid 4352)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:38:21.333 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python neural_net_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99751661-283c-499f-8146-280f87f6e965",
   "metadata": {},
   "source": [
    "And now the cards can be visualized for each step. In this case, lets look at the card associated with the `visualize_feature_distributions` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcf0e712-f607-4cce-8d7c-cf19d51eab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mNeuralNetFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[22mResolving card: NeuralNetFlow/1659418681517256/visualize_feature_distributions/3\u001b[K\u001b[0m\u001b[32m\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python neural_net_flow.py card view visualize_feature_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd3c5c-7d5b-4fba-b60f-4851d210b631",
   "metadata": {},
   "source": [
    "## Episode 4. Analyze Flow Results Using Client API\n",
    "\n",
    "### Showcasing\n",
    "- The Metaflow Client API\n",
    "- Tagging, filtering, and accessing data from flows\n",
    "\n",
    "Cards are handy for quick visualizations and genering report elements from flows. For more involved analysis of a flow's run history, you can use the Metaflow Client API. The Client offers ways to tag, filter, and access data from flows. For example, you can access the results of the latest run of `TreeModelsFlow` from episode 2 like:\n",
    "\n",
    "\n",
    "[HBA: should we talk about whether to do this in notebooks or from the terminal or?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3a75590-f75a-4b3a-90de-094e7d20deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "tree_flow_run = Flow('TreeModelsFlow').latest_run\n",
    "assert tree_flow_run.successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a00e9c-83df-4b2f-8699-d5a0892b2157",
   "metadata": {},
   "source": [
    "Once you have fetched the run, you can do things like:\n",
    "* add, drop, or edit tags to the run\n",
    "* view DAG structure\n",
    "* view artifact state throughout steps\n",
    "* view metadata about the run\n",
    "\n",
    "For example, you can access any artifact stored using `self` with `<RUN NAME>.data.<ARTIFACT NAME>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73f021ff-618f-4789-bf8c-ccd6290177b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Random Forest', 'XGBoost'], [1.0, 1.0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_flow_run.data.model_names, tree_flow_run.data.accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca423c9-9429-4119-a97d-90eb62f22da6",
   "metadata": {},
   "source": [
    "Let's compare the accuracy scores from each of the two tree models and the neural network in episode 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd5a9128-ed7a-4a37-87c2-9b63d0791241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "XGBoost Accuracy: 1.0\n",
      "Neural Net Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from metaflow import Flow\n",
    "tree_flow_run = Flow('TreeModelsFlow').latest_run\n",
    "neural_net_run = Flow('NeuralNetFlow').latest_run\n",
    "\n",
    "for model_name, acc in zip(\n",
    "    [*tree_flow_run.data.model_names, \"Neural Net\"],\n",
    "    [*tree_flow_run.data.accuracies, neural_net_run.data.accuracy]\n",
    "):\n",
    "    print(\"{} Accuracy: {}\".format(model_name, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ad538-f32c-4516-9356-e6c94ecc31e6",
   "metadata": {},
   "source": [
    "## Episode 5. Debugging Flows\n",
    "\n",
    "### Showcasing\n",
    "- Metaflow `resume`\n",
    "- Debugging flows\n",
    "\n",
    "The team behind Metaflow wants you to have a great experience working with Metaflow. To this end, debugging is a first-class workflow in the Metaflow developer experience. In this episode, we focus on using `resume` in the command line when debugging your flows. The general structure of using this command is like: \n",
    "* Write `my_sweet_flow.py`\n",
    "* Run `python my_sweet_flow.py run`\n",
    "    * Oh no, something broke! Analyzing stack trace...\n",
    "    * Found the bug! \n",
    "    * Save `my_sweet_flow.py` with the fix. \n",
    "* `python my_sweet_flow.py resume`\n",
    "    * Pick up the state of the last flow execution *from the step that failed*.\n",
    "    * Note: You can also specify a specific step to resume from like `python my_sweet_flow.py resume <DIFFERENT STEP NAME>`\n",
    "    \n",
    "Let's look at an example. In this flow:\n",
    "* The `time_consuming_step` mimics some process you'd rather not re-run because of a downstream error.\n",
    "* The `error_prone_step` creates an `Exception` that halts your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03ff887e-4fe5-4136-891f-b36837b74ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting debuggable_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile debuggable_flow.py\n",
    "from metaflow import FlowSpec, step\n",
    "\n",
    "class DebuggableFlow(FlowSpec):\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.time_consuming_step)\n",
    "        \n",
    "    @step\n",
    "    def time_consuming_step(self):\n",
    "        import time\n",
    "        time.sleep(12)\n",
    "        self.next(self.error_prone_step)\n",
    "        \n",
    "    @step\n",
    "    def error_prone_step(self):\n",
    "        raise Exception()\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Flow is done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DebuggableFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8068bd5-91c8-44fd-80ac-90db9139bc85",
   "metadata": {},
   "source": [
    "If you run this flow using the following command, the `error_prone_step` will produce an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "734ea1a4-bad4-48b5-a0b1-9724a7dfc486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mDebuggableFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:11.657 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418751653854):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:11.665 \u001b[0m\u001b[32m[1659418751653854/start/1 (pid 4366)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:12.059 \u001b[0m\u001b[32m[1659418751653854/start/1 (pid 4366)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:12.069 \u001b[0m\u001b[32m[1659418751653854/time_consuming_step/2 (pid 4369)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.537 \u001b[0m\u001b[32m[1659418751653854/time_consuming_step/2 (pid 4369)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.545 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.881 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22m<flow DebuggableFlow step error_prone_step> failed:\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.884 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mInternal error\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.885 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.885 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/cli.py\", line 1160, in main\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.885 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mstart(auto_envvar_prefix=\"METAFLOW\", obj=state)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.885 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/core.py\", line 829, in __call__\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.885 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mreturn self.main(args, kwargs)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.925 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/core.py\", line 782, in main\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mrv = self.invoke(ctx)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/core.py\", line 1259, in invoke\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mreturn _process_result(sub_ctx.command.invoke(sub_ctx))\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/core.py\", line 1066, in invoke\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mreturn ctx.invoke(self.callback, ctx.params)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/core.py\", line 610, in invoke\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mreturn callback(args, kwargs)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/_vendor/click/decorators.py\", line 21, in new_func\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mreturn f(get_current_context(), args, kwargs)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/cli.py\", line 569, in step\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mtask.run_step(\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/task.py\", line 585, in run_step\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mself._exec_step_function(step_func)\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/opt/anaconda3/envs/full-stack-metaflow/lib/python3.9/site-packages/metaflow/task.py\", line 59, in _exec_step_function\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mstep_function()\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mFile \"/Users/eddie/Dev/tutorials/nbs/intro-to-mf/debuggable_flow.py\", line 17, in error_prone_step\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mraise Exception()\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22mException\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.926 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.927 \u001b[0m\u001b[32m[1659418751653854/error_prone_step/3 (pid 4372)] \u001b[0m\u001b[1mTask failed.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.927 \u001b[0m\u001b[31m\u001b[1mWorkflow failed.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.927 \u001b[0m\u001b[31m\u001b[1mTerminating 0 active tasks...\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:24.927 \u001b[0m\u001b[31m\u001b[1mFlushing logs...\u001b[0m\n",
      "\u001b[1m    Step failure\u001b[0m\u001b[22m:\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    Step \u001b[0m\u001b[31m\u001b[1merror_prone_step\u001b[0m\u001b[22m (task-id 3) failed.\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m\u001b[K\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python debuggable_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431dd52c-f878-44cd-a3b6-ffe2fd845619",
   "metadata": {},
   "source": [
    "You can resume from the step that failed by:\n",
    "1. Finding and fixing the bug. \n",
    "2. Saving the flow script.\n",
    "3. Running `python <FLOW SCRIPT> resume`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d63beca1-c5c3-44aa-bdfc-47c72f396d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting debuggable_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile debuggable_flow.py\n",
    "from metaflow import FlowSpec, step\n",
    "\n",
    "class DebuggableFlow(FlowSpec):\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.time_consuming_step)\n",
    "        \n",
    "    @step\n",
    "    def time_consuming_step(self):\n",
    "        import time\n",
    "        time.sleep(12)\n",
    "        self.next(self.error_prone_step)\n",
    "        \n",
    "    @step\n",
    "    def error_prone_step(self):\n",
    "        print(\"Squashed bug\")\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Flow is done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DebuggableFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a466c81-460b-4be3-9e8b-ba94584c3709",
   "metadata": {},
   "source": [
    "When you run the following command, notice the console print outs will contain notes about previous run tasks being cloned and the flow doesn't wait on `time_consuming_step` since we resumed downstream of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2df38468-63e1-4d20-aece-d4e54946805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.1\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mDebuggableFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:36.594 \u001b[0m\u001b[22mGathering required information to resume run (this may take a bit of time)...\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:36.599 \u001b[0m\u001b[1mWorkflow starting (run-id 1659418776593706):\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:36.600 \u001b[0m\u001b[32m[1659418776593706/start/1] \u001b[0m\u001b[1mCloning results of a previously run task 1659418751653854/start/1\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:36.972 \u001b[0m\u001b[32m[1659418776593706/time_consuming_step/2] \u001b[0m\u001b[1mCloning results of a previously run task 1659418751653854/time_consuming_step/2\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:37.365 \u001b[0m\u001b[32m[1659418776593706/error_prone_step/3 (pid 4399)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:37.706 \u001b[0m\u001b[32m[1659418776593706/error_prone_step/3 (pid 4399)] \u001b[0m\u001b[22msquashed bug\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:37.752 \u001b[0m\u001b[32m[1659418776593706/error_prone_step/3 (pid 4399)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:37.760 \u001b[0m\u001b[32m[1659418776593706/end/4 (pid 4402)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:38.108 \u001b[0m\u001b[32m[1659418776593706/end/4 (pid 4402)] \u001b[0m\u001b[22mFlow is done!\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:38.158 \u001b[0m\u001b[32m[1659418776593706/end/4 (pid 4402)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-08-02 00:39:38.159 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python debuggable_flow.py resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e67a008-c9f2-4194-95a4-a915b426744a",
   "metadata": {},
   "source": [
    "Note that this same funcionality works even when the steps are run on different computers. In fact, you can even resume a Metaflow run on your local machine for a flow that was run on a production scheduler like AWS Step Functions or Argo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8385169-c2b3-4418-a2c2-4709c3fefde9",
   "metadata": {},
   "source": [
    "### Lesson Close\n",
    "Way to stick with this lesson through all five episodes. You are going to be a great Metaflower. If you are ready for more, check out the next lesson in this tutorial where we move to the cloud. Hope to see you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
