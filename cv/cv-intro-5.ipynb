{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b468bf5b-51b3-4faa-9928-ecd5291b1b8f",
   "metadata": {},
   "source": [
    "---\n",
    "title: Computer Vision - Episode 5\n",
    "slug: /docs/cv-tutorial-L5/\n",
    "tags: [data, cv]\n",
    "sidebar_label: Interpret Results\n",
    "id: cv-tutorial-L5\n",
    "pagination_next: tutorials/nbs/cv/cv-tutorial-L6\n",
    "pagination_prev: tutorials/nbs/cv/cv-tutorial-L4\n",
    "description: A tutorial that uses Keras, Scikit-Learn, and Metaflow to operationalize a machine learning workflow.\n",
    "category: data science\n",
    "hide_table_of_contents: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ecaed-2962-4051-a2d1-a0b1b622fbfa",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "Please follow the [setup instructions](/docs/cv-tutorial-setup) to prepare your environment if you haven't yet.  \n",
    "This tutorial will be referencing this [Notebook](https://github.com/outerbounds/tutorials/blob/main/cv/cv-intro-5.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502c114-7449-4d4a-aaf3-738834ef6f55",
   "metadata": {},
   "source": [
    "[Tagging](https://docs.metaflow.org/scaling/tagging#tagging) allows you to categorize and organize flows, which we can use to mark certain models however we wish. This can be done via the Metaflow UI or programmatically. Tagging can be useful in situations like determining which models are production candidates and analyzing which model architectures converge."
   ]
  },
  {
   "cell_type": "raw",
   "id": "33a0721b-4d44-4e5c-83c1-56600bbfdec3",
   "metadata": {},
   "source": [
    "### <NumberHeading number={1}>Load Flow Results</NumberHeading>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2912ab-6d72-428e-8c4f-a0d93e8cadb1",
   "metadata": {},
   "source": [
    "Since tagging is fundamentally about interpreting the results of flows, lets start by loading run data from the `TuningFlow` you built in [lesson 4](/docs/cv-tutorial-L4). The data can be accessed in any Python environment using Metaflow's Client API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3da181-4c0e-4d64-9385-66e80d7db4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metaflow import Flow\n",
    "model_comparison_flow = Flow('ModelComparisonFlow')\n",
    "tuning_flow = Flow('TuningFlow')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65a8524f-8abf-41fe-9bd6-fe505909f8a0",
   "metadata": {},
   "source": [
    "### <NumberHeading number={2}>Define How to Aggregate and Compare Results</NumberHeading>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd435a-34d8-4233-96ce-3d4cd9c14bd2",
   "metadata": {},
   "source": [
    "Next we define a function to parse the data in the runs. \n",
    "This `add_stats` function will progressively build up a dictionary called `stats`.\n",
    "Each new entry in the `stats` dictionary contains hyperparameters, metrics, and metadata corresponding to a model trained in a `TuningFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a85a720-1f1a-422d-8f91-ef4749825cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(stats, run):\n",
    "    if run.successful and hasattr(run.data, 'results'):\n",
    "        results = run.data.results\n",
    "        best_run = results.iloc[results['test accuracy'].idxmax()]\n",
    "        stats['flow id'].append(run.id)\n",
    "        stats['flow name'].append(run.parent.pathspec)\n",
    "        stats['model name'].append(best_run['model'])\n",
    "        stats['test accuracy'].append(best_run['test accuracy'])\n",
    "        stats['test loss'].append(best_run['test loss'])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9063e8-6e36-41be-b73f-6e0ded7cc868",
   "metadata": {},
   "source": [
    "Next we loop through runs of `TuningFlow` and `ModelComparisonFlow` and aggregate `stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd75d86a-e1de-4f25-88da-0c896a725bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    'flow id': [],\n",
    "    'flow name': [],\n",
    "    'model name': [],\n",
    "    'test accuracy': [],\n",
    "    'test loss': []\n",
    "}\n",
    "\n",
    "for run in tuning_flow.runs():\n",
    "    stats = get_stats(stats, run)\n",
    "    \n",
    "for run in model_comparison_flow.runs():\n",
    "    stats = get_stats(stats, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d2386b-5af0-4916-9ebc-323660cf1c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow id</th>\n",
       "      <th>flow name</th>\n",
       "      <th>model name</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1665537445969331</td>\n",
       "      <td>TuningFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.041102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1665536981222257</td>\n",
       "      <td>ModelComparisonFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>0.026922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1665533644512368</td>\n",
       "      <td>ModelComparisonFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.025901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1665532478711797</td>\n",
       "      <td>ModelComparisonFlow</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.031949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            flow id            flow name model name  test accuracy  test loss\n",
       "0  1665537445969331           TuningFlow        CNN         0.9866   0.041102\n",
       "1  1665536981222257  ModelComparisonFlow        CNN         0.9913   0.026922\n",
       "2  1665533644512368  ModelComparisonFlow        CNN         0.9917   0.025901\n",
       "3  1665532478711797  ModelComparisonFlow        CNN         0.9894   0.031949"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = pd.DataFrame(stats)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebcd35be-a7b3-4044-9229-a18126a0cf63",
   "metadata": {},
   "source": [
    "### <NumberHeading number={3}>Access the Best Model</NumberHeading>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994b87e-bd07-487e-acc5-1b0b04a37b41",
   "metadata": {},
   "source": [
    "With the list of `best_models`, we can sort by `test accuracy` performance and find the run containing the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1c1725-e1c5-4388-9438-4ee54131ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run('ModelComparisonFlow/1665533644512368')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metaflow import Run\n",
    "sorted_models = best_models.sort_values(by='test accuracy', ascending=False).iloc[0]\n",
    "run = Run(\"{}/{}\".format(sorted_models['flow name'], sorted_models['flow id']))\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca9401-9f2e-49b5-8701-ff45a5381e99",
   "metadata": {},
   "source": [
    "Next, the model can be used to make predictions that we can check make sense next to the true targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ba1b47-c141-44f8-a06f-02878e76f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# get data samples\n",
    "((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()\n",
    "x_test = np.expand_dims(x_test.astype(\"float32\") / 255, -1)\n",
    "\n",
    "# use best_model from the Metaflow run\n",
    "logits = run.data.best_model.predict(x_test)\n",
    "softmax = keras.layers.Softmax(axis=1)\n",
    "probs = softmax(logits).numpy()\n",
    "pred = probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b5ae773-d539-4974-99cf-957de4aa06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicts [7 2 1 ... 4 5 6]\n",
      "  True targets [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model predicts {}\".format(pred))\n",
    "print(\"  True targets {}\".format(y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b6cbb75-ab90-41c0-bba6-169fa143cca6",
   "metadata": {},
   "source": [
    "### <NumberHeading number={4}>Interpret Results with Tags</NumberHeading>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf1733-1d17-4fa5-b133-e5792977523b",
   "metadata": {},
   "source": [
    "Now that you can search through the flows and model's trained in them, it is time to leverage tagging. \n",
    "You can `add_tag` on runs that meet any condition you find suitable.\n",
    "In this case we consider models that have a `test accuracy` above `threshold = 0.985`. \n",
    "Runs that have models meeting this threshold are tagged as `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87b9a59f-c9c4-4a7e-8993-008e78cc9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.985\n",
    "for run in tuning_flow:\n",
    "    if run.successful and hasattr(run.data, 'results'):\n",
    "        if run.data.results['test accuracy'].max() > threshold:\n",
    "            run.add_tag('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a52d9-636d-49bf-a9a0-e43649ed9ffe",
   "metadata": {},
   "source": [
    "Now runs can be accessed by filtering on this tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8068c2a8-fb39-46fc-80d2-ccd4f81ffa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "production_runs = Flow('TuningFlow').runs('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2595ad-d520-4065-bf4d-6e9a906aa54f",
   "metadata": {},
   "source": [
    "In this lesson you saw how to load and analyze results of your flows. \n",
    "You added tags to runs that met your requirements for production quality.\n",
    "In the next lesson, you will see how to use the models that meet these requirements in a prediction flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
