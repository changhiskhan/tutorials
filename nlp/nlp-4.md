---
title: Episode 4
slug: /docs/nlp-tutorial-L4
tags: [orchestration]
sidebar_label: Episode 4
id: nlp-tutorial-L4
description: A tutorial that uses Keras, Scikit-Learn, and Metaflow to operationalize a machine learning workflow.
category: tutorials
---


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

### Setup Instructions

If you haven't done so, please follow the [setup instructions](./nlp-tutorial-setup) to prepare your environment.  This tutorial will references the python script [branchflow.py](https://github.com/outerbounds/tutorials/blob/main/nlp/branchflow.py).

#### What You Will Learn
At the end of this lesson, you will:
    
* Learn how to refactor a training code into a Flow.
* Learn how to compute steps in parallel with branching, which we have seen before in the [intro tutorial](../docs/intro-tutorial-S2E3).

In Lesson 3, you saw how we constructed a basic flow to compute the baseline for our NLP task.  In this lesson, we will learn how to incorporate the model as well as show you how to use branching to compute things in parallel.

### <NumberHeading number={1}>What is Branching</NumberHeading>


[Branching](https://docs.metaflow.org/metaflow/basics#branch) is a powerful feature in Metaflow that allows you complete steps in parallel instead of a linear fashion.  To demonstrate this feature, we will construct our `baseline` and `train` steps as two branches that will execute in parallel.  It should be noted that anytime you use branching, you also need a `join` step to disambiguate the branches, which you can [read more about here](https://docs.metaflow.org/metaflow/basics#branch).

<Wrapper>

### <NumberHeading number={2}>Write A Flow</NumberHeading>

<Highlight>


In this flow, we will modify the `start`  and `join` steps to achieve branching, as well as add a `train` step that will train our model.


</Highlight>


Below is a detailed explanation of the changes we are making to our original flow:

1. **Create a branching workflow to create a baseline and candidate model in parallel**  in the `baseline` and `train` steps.
    - When we call `self.next(self.baseline, self.train)`,  this creates a [branching flow](https://docs.metaflow.org/metaflow/basics#branch) that will allow the `baseline` and  `train` steps to run in parallel.
2. **Add a training step** The `train` step uses a neural-bag-of-words model to train a text classifier.  
    - We import the `NbowModel` module we created in Lesson 1.
    - We save this model in a special way by setting the `model_dict` property of our custom model to `self.model_dict`, which has the effect of storing this data in Metaflow's artifact store, where data is versioned and saved automatically.
3. **Add a join step**: In this step, we will load our model using `NbowModel.from_dict(self.model_dict)` as well as disambiguate the data in our branches.
    - The join step can disambiguate data by referring to a specific step in the branch. For example, `inputs.start.df` refers to the `start` step, and specifially the `df` artifact stored in that step.
    - We print the performance metrics of our model and the baseline in this join step.

<RHS>



```py title="branchflow.py"
from metaflow import FlowSpec, step, Flow, current

class BranchNLPFlow(FlowSpec):
        
    @step
    def start(self):
        "Read the data"
        import pandas as pd
        self.df = pd.read_parquet('train.parquet')
        self.valdf = pd.read_parquet('valid.parquet')
        print(f'num of rows: {self.df.shape[0]}')
        self.next(self.baseline, self.train)

    @step
    def baseline(self):
        "Compute the baseline"
        from sklearn.metrics import accuracy_score, roc_auc_score
        baseline_predictions = [1] * self.valdf.shape[0]
        self.base_acc = accuracy_score(self.valdf.labels, baseline_predictions)
        self.base_rocauc = roc_auc_score(self.valdf.labels, baseline_predictions)
        self.next(self.join)

    @step
    def train(self):
        "Train the model"
        from model import NbowModel
        model = NbowModel(vocab_sz=750)
        model.fit(X=self.df['review'], y=self.df['labels'])
        self.model_dict = model.model_dict #save model
        self.next(self.join)
        
    @step
    def join(self, inputs):
        "Compare the model results with the baseline."
        import pandas as pd
        from model import NbowModel
        self.model_dict = inputs.train.model_dict
        self.train_df = inputs.train.df
        self.val_df = inputs.baseline.valdf
        self.base_rocauc = inputs.baseline.base_rocauc
        self.base_acc = inputs.baseline.base_acc
        model = NbowModel.from_dict(self.model_dict)
        
        self.model_acc = model.eval_acc(X=self.val_df['review'], labels=self.val_df['labels'])
        self.model_rocauc = model.eval_rocauc(X=self.val_df['review'], labels=self.val_df['labels'])
        
        print(f'Baseline Acccuracy: {self.base_acc:.2%}')
        print(f'Baseline AUC: {self.base_rocauc:.2}')
        print(f'Model Acccuracy: {self.model_acc:.2%}')
        print(f'Model AUC: {self.model_rocauc:.2}')
        self.next(self.end)
        
    @step
    def end(self):
        print('Flow is complete')
        

if __name__ == '__main__':
    BranchNLPFlow()
```

</RHS>


### <NumberHeading number={3}>Run Flow</NumberHeading>



```bash
python branchflow.py run
```

<CodeOutputBlock lang="bash">

```
     Workflow starting (run-id 1660240972126592):
     [1660240972126592/start/1 (pid 30908)] Task is starting.
     [1660240972126592/start/1 (pid 30908)] num of rows: 20377
     [1660240972126592/start/1 (pid 30908)] Task finished successfully.
     [1660240972126592/baseline/2 (pid 30912)] Task is starting.
     [1660240972126592/train/3 (pid 30913)] Task is starting.
     [1660240972126592/baseline/2 (pid 30912)] Task finished successfully.
     [1660240972126592/train/3 (pid 30913)] 139: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2
     [1660240972126592/train/3 (pid 30913)] Epoch 1/10
    510/510 [==============================] - 1s 1ms/step - loss: 0.3614 - accuracy: 0.8460 - val_loss: 0.2983 - val_accuracy: 0.873703 - loss: 0.8357 - accuracy: 0.25
     [1660240972126592/train/3 (pid 30913)] Epoch 2/10
    510/510 [==============================] - 1s 1ms/step - loss: 0.2944 - accuracy: 0.8817 - val_loss: 0.2955 - val_accuracy: 0.8759 - loss: 0.2574 - accuracy: 0.87
     [1660240972126592/train/3 (pid 30913)] Epoch 3/10
    510/510 [==============================] - 1s 991us/step - loss: 0.2833 - accuracy: 0.8865 - val_loss: 0.2959 - val_accuracy: 0.8768 loss: 0.2888 - accuracy: 0.90
     [1660240972126592/train/3 (pid 30913)] Epoch 4/10
    510/510 [==============================] - 1s 996us/step - loss: 0.2744 - accuracy: 0.8897 - val_loss: 0.3004 - val_accuracy: 0.8710 loss: 0.3457 - accuracy: 0.87
     [1660240972126592/train/3 (pid 30913)] Epoch 5/10
    510/510 [==============================] - 1s 991us/step - loss: 0.2640 - accuracy: 0.8974 - val_loss: 0.3008 - val_accuracy: 0.8756 loss: 0.1472 - accuracy: 0.93
     [1660240972126592/train/3 (pid 30913)] Epoch 6/10
    510/510 [==============================] - 0s 968us/step - loss: 0.2542 - accuracy: 0.9032 - val_loss: 0.3061 - val_accuracy: 0.8732 loss: 0.3837 - accuracy: 0.90
     [1660240972126592/train/3 (pid 30913)] Epoch 7/10
    510/510 [==============================] - 0s 961us/step - loss: 0.2469 - accuracy: 0.9088 - val_loss: 0.3086 - val_accuracy: 0.8732 loss: 0.1132 - accuracy: 1.00
     [1660240972126592/train/3 (pid 30913)] Epoch 8/10
    510/510 [==============================] - 0s 977us/step - loss: 0.2380 - accuracy: 0.9138 - val_loss: 0.3151 - val_accuracy: 0.8746 loss: 0.2487 - accuracy: 0.84
     [1660240972126592/train/3 (pid 30913)] Epoch 9/10
    510/510 [==============================] - 0s 959us/step - loss: 0.2275 - accuracy: 0.9190 - val_loss: 0.3234 - val_accuracy: 0.8719 loss: 0.2535 - accuracy: 0.90
     [1660240972126592/train/3 (pid 30913)] Epoch 10/10
    510/510 [==============================] - 1s 1ms/step - loss: 0.2183 - accuracy: 0.9252 - val_loss: 0.3345 - val_accuracy: 0.8641 - loss: 0.1961 - accuracy: 0.93
     [1660240972126592/train/3 (pid 30913)] To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
     [1660240972126592/train/3 (pid 30913)] 234: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
     [1660240972126592/train/3 (pid 30913)] Task finished successfully.
     [1660240972126592/join/4 (pid 30921)] Task is starting.
     [1660240972126592/join/4 (pid 30921)] 336: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2
     [1660240972126592/join/4 (pid 30921)] Baseline Acccuracy: 77.30%
     [1660240972126592/join/4 (pid 30921)] To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
     [1660240972126592/join/4 (pid 30921)] 122: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
     [1660240972126592/join/4 (pid 30921)] Baseline AUC: 0.5
     [1660240972126592/join/4 (pid 30921)] Model Acccuracy: 87.10%
     [1660240972126592/join/4 (pid 30921)] Model AUC: 0.91
     [1660240972126592/join/4 (pid 30921)] Task finished successfully.
     [1660240972126592/end/5 (pid 30925)] Task is starting.
     [1660240972126592/end/5 (pid 30925)] Flow is complete
     [1660240972126592/end/5 (pid 30925)] Task finished successfully.
     Done!
```

</CodeOutputBlock>

</Wrapper>


### Next Steps

We can see from the Metaflow logs that our model looks promising in that it is performing better than the baseline!  However, computing the baseline isn't just meant for the logs!  We should use the baseline alongside other tests to gate which models make it to production.

In the next lesson, you will learn how to test our models and use tagging to manage which models are promoted to production.


